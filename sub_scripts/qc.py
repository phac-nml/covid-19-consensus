#!/usr/bin/env python3

# Dependencies
import csv
import vcf
import re

import pandas as pd
from pybedtools import BedTool
from functools import reduce

def get_vcf_variants(variants_vcf, variants_list=[], locations=[]):
    '''
    Use pipeline pass.vcf.gz file to get passing variants and their locations
    INPUTS:
        variants_vcf   --> `path` from argparse to the input vcf.gz file
        variants_list  --> `list` to append the variants formated as RefPosAlt
                        ['G231T', 'A3423C']
        locations      --> `list` to append only the variant location ints to
    RETURNS:
        'None' or str variants separated by a ;
        location list or None for the variant int location
    '''
    vcf_reader = vcf.Reader(open(variants_vcf, 'rb'))
    for rec in vcf_reader:
        variant = '{}{}{}'.format(rec.REF, rec.POS, rec.ALT[0])

        # Checking for duplicate variants that have been an issue
        if variant in variants_list:
            pass
        # Removal of N variants for consensus script
        elif rec.ALT[0] == 'N':
            pass
        else:
            variants_list.append(variant)
            locations.append(rec.POS)

    variants = (';'.join(variants_list))

    if variants == '':
        return 'None', None

    return variants, locations


def get_tsv_variants(variants_tsv, variants_list=[], locations=[]):
    '''
    Use pipeline variants.tsv file to get passing variants and their locations
    INPUTS:
        variants_tsv   --> `path` from argparse to the input tsv file generated by signal
        variants_list  --> `list` to append the variants formated as RefPosAlt
                        ['G231T', 'A3423C']
        locations      --> `list` to append only the variant location ints to
    RETURNS:
        `str` of 'None' or variants separated by ;
        `list` of locations or None if no variants found
    '''
    with open(variants_tsv) as input_handle:

        for index, line in enumerate(input_handle):

            # Pass the header line so as to not include it!
            if index == 0:
                continue

            row = line.strip('\n').split('\t') # Order is [REGION, POS, REF, ALT, ...]

            variant = '{}{}{}'.format(row[2], row[1], row[3])

            # Checking for duplicate variants that have been an issue
            if variant in variants_list:
                pass
            else:
                variants_list.append(variant)
                locations.append(row[1])
    
    variants = (';'.join(variants_list))

    if variants == '':
        return 'None', None
        
    return variants, locations

def find_primer_mutations(pcr_bed, genomic_locations, primer_mutations=[]):
    '''
    Use variant info to check for mutations in currently used primers
    INPUTS:
        pcr_bed            --> `path` from argparse to input bed file of PCR primers
        genomic_locations  --> `list` of integer locations to check or `None` if none found
        primer_mutations   --> `list` to append any primer mutations found
    RETURNS:
        `str` primer mutations statement
    '''
    if genomic_locations is None:
        return 'None'
    
    input_bed = BedTool(pcr_bed)

    for gene in input_bed:
        location = range(gene.start, gene.stop + 1) # Plus one to make sure that we get mutations in the final location of the range

        for variant_pos in genomic_locations:
            if variant_pos in location:
                primer_mutations.append('Variant position {} overlaps PCR primer {}'.format(variant_pos, gene.name))
    
    if primer_mutations != []:
        statement = '; '.join(primer_mutations)
        return 'Warning: {}'.format(statement)

    return 'None'

def get_lineage(pangolin_csv, sample_name):
    '''
    Check Pangolin output for the lineage of the sample
    INPUTS:
        pangolin_csv --> `path` from argparse to input pangolin csv file
        sample_name  --> `str` sample name from argparse
    RETURNS:
        `str` lineage
        `str` pangoLEARN version
    '''
    with open(pangolin_csv, 'r') as input_handle:
        reader = csv.reader(input_handle)

        for row in reader: # Row format is ['taxon', 'lineage', 'probability', 'pangoLEARN_version', 'status', 'note']

            if re.search(sample_name, row[0]):
                return str(row[1]), str(row[3])
    
    return 'Unknown', 'Unknown'

def get_protein_variants(aa_table):
    '''
    Parse ncov-tools output to report its amino acid mutations
    INPUTS:
        aa_table  --> `path` from argparse to input aa_table.csv
    RETURNS:
        `str` amino acid mutations separated by a ;
    '''
    df = pd.read_csv(aa_table, sep='\t').dropna()
    df = df[~df.alt.str.contains("N")]
    return ';'.join(df['aa'].tolist())

def parse_ncov_tsv(file_in, sample):
    '''
    Parse ncov-tools output tsv files (summary and negative) to grab data for the sample
    INPUTS:
        file_in   --> `path` from argparse to input ncov-tools tsv file to parse
        sample    --> `str` sample name from argparse
        negative  --> `boolean` for if the tsv table is for the negative control or not
    RETURNS:
        Populated `df`
    '''
    # Read in file
    df = pd.read_csv(file_in, sep='\t')

    # Drop lineage column as we have asked gotten that ourselves
    df.drop(columns=['lineage'], inplace=True)

    # Set which column contains the sample
    sample_column = 'sample'

    # Finding the data, all samples will be in ncov-tools summary output (as they had data generated)
    for index, name in enumerate(df[sample_column].tolist()):
        if re.search(sample, name):
            df.loc[index, sample_column] = sample
            df.fillna('NA', inplace=True)
            return df.iloc[[index]]
    
    # If nothing is found we can just skip by giving an empty df with the sample
    no_df = pd.DataFrame(columns=[sample_column])
    no_df.loc[1, sample_column] = sample
    
    return no_df

def get_samplesheet_info(sample_tsv, sample_name):
    '''
    Parse samplesheet info to allow for IRIDA uploads and adding whatever data wanted to output qc file
    INPUTS:
        sample_tsv   --> `path` from argparse to input samplesheet.tsv file
        sample_name  --> `str` sample name from argparse
    RETURNS:
        `df` populated with data from samplesheet
    '''
    df = pd.read_csv(sample_tsv, sep='\t', dtype=object)
    # ncov-tools captures these columns and date is not required so drop them
    try:
        df.drop(columns=['ct', 'date'], inplace=True)
    except KeyError:
        pass
    
    # Get only the sample row and if empty, fill it in to match other rows
    df = df.loc[df['sample'] == sample_name]
    if df.empty:
        df.loc[1, 'sample']  = sample_name
    
    df.fillna('NA', inplace=True)

    return df

def run(args):

    ### Added checks ###
    ####################
    if args.nanopore:
        # Vcf passing variants from nanopore pipeline
        variants, variant_locations = get_vcf_variants(args.vcf)
    
    elif args.illumina:
        # Tsv variants from Illumina pipeline
        if args.tsv_variants:
            variants, variant_locations = get_tsv_variants(args.tsv_variants)
        # VCF variants from FREEBAYES
        else:
            variants, variant_locations = get_vcf_variants(args.vcf)

    # Find any overlap of variants in the pcr primer regions
    primer_statement = find_primer_mutations(args.pcr_bed, variant_locations)

    # Pangolin Lineages
    lineage, pangoLearn = get_lineage(args.pangolin, args.sample)

    # snpEFF output
    protein_variants = get_protein_variants(args.snpeff_tsv)

    # NCOV-Tools Results
    summary_df = parse_ncov_tsv(args.ncov_summary, args.sample)

    # If we have a samplesheet, use its values to create final output
    if args.sample_sheet:
        sample_sheet_df = get_samplesheet_info(args.sample_sheet, args.sample)

        qc_line = {  'sample' : [args.sample],
                    'lineage' : [lineage],
                   'variants' : [variants],
            'pangoLEARN_version' : [pangoLearn],
            'protein_variants': [protein_variants],
'diagnostic_primer_mutations' : [primer_statement],
                'script_name' : [args.script_name],
                   'revision' : [args.revision]}
        
        qc_df = pd.DataFrame.from_dict(qc_line)
        data_frames = [sample_sheet_df, qc_df, summary_df]

    else:
        qc_line = {      'sample' : [args.sample],
                        'lineage' : [lineage],
                       'variants' : [variants],
                'pangoLEARN_version' : [pangoLearn],
                'protein_variants': [protein_variants],
    'diagnostic_primer_mutations' : [primer_statement],
                    'script_name' : [args.script_name],
                       'revision' : [args.revision]}

        qc_df = pd.DataFrame.from_dict(qc_line)

        data_frames = [qc_df, summary_df]

    # Merge all dataframes together
    out_df = reduce(lambda left,right: pd.merge(left,right,on='sample', how='left'), data_frames)

    # Remove comma's as some of the ncov-tools fields have commas
    out_df.replace(',',';', regex=True, inplace=True)

    # Output
    out_df.to_csv(args.outfile, sep=',', index=False)

def main():
    import argparse

    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--nanopore', action='store_true')
    group.add_argument('--illumina', action='store_true')
    parser.add_argument('--outfile', required=True)
    parser.add_argument('--sample', required=True)
    parser.add_argument('--pangolin', required=True)
    parser.add_argument('--ncov_summary', required=True)
    parser.add_argument('--revision', required=True)
    parser.add_argument('--script_name', required=True)
    parser.add_argument('--vcf', required=False)
    parser.add_argument('--tsv_variants', required=False)
    parser.add_argument('--snpeff_tsv', required=True)
    parser.add_argument('--pcr_bed', required=True)
    parser.add_argument('--sample_sheet', required=False)

    args = parser.parse_args()
    run(args)

if __name__ == "__main__":
    main()
